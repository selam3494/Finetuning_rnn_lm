{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12669115,"sourceType":"datasetVersion","datasetId":8006116}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This notebook demonstrates reinforcement learning with human feedback (RLHF) and PPO-based reward model optimization.","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:49:04.503202Z","iopub.execute_input":"2025-08-04T14:49:04.503799Z","iopub.status.idle":"2025-08-04T14:49:04.507789Z","shell.execute_reply.started":"2025-08-04T14:49:04.503774Z","shell.execute_reply":"2025-08-04T14:49:04.507106Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# !pip install trl\n# !pip install --upgrade bitsandbytes accelerate\n\nimport random\nimport numpy as np\nimport torch \nimport json\nimport torch\nimport pandas as pd\nimport datasets\nimport time\nfrom torch import nn\nfrom datasets import load_dataset, Dataset\nfrom torch.utils.data import Dataset\nfrom random import choices\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    pipeline,\n    DataCollatorForLanguageModeling,\n)\nfrom trl import (\n    RewardTrainer, \nRewardConfig,\n    SFTTrainer,\n    PPOConfig,\n    PPOTrainer,\n    AutoModelForCausalLMWithValueHead,\n    create_reference_model\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:21:38.540102Z","iopub.execute_input":"2025-08-04T14:21:38.540683Z","iopub.status.idle":"2025-08-04T14:21:52.205839Z","shell.execute_reply.started":"2025-08-04T14:21:38.540657Z","shell.execute_reply":"2025-08-04T14:21:52.204997Z"}},"outputs":[{"name":"stderr","text":"2025-08-04 14:21:46.048829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754317306.073480     443 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754317306.081057     443 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA device count:\", torch.cuda.device_count())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:26:39.176130Z","iopub.execute_input":"2025-08-04T14:26:39.176516Z","iopub.status.idle":"2025-08-04T14:26:39.181491Z","shell.execute_reply.started":"2025-08-04T14:26:39.176489Z","shell.execute_reply":"2025-08-04T14:26:39.180582Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nCUDA device count: 2\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Train Policy Model for human evaluation","metadata":{}},{"cell_type":"code","source":"def set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:22.186120Z","iopub.execute_input":"2025-08-04T14:22:22.186410Z","iopub.status.idle":"2025-08-04T14:22:22.190610Z","shell.execute_reply.started":"2025-08-04T14:22:22.186390Z","shell.execute_reply":"2025-08-04T14:22:22.189896Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"output_dir = \"/kaggle/working/supervised-summarize-checkpoint\"\ntrain_batch_size = 16\ngradient_accumulation_steps = 1\nlearning_rate = 1e-5\neval_batch_size = 4\neval_steps = 500\nmax_input_length = 512\nsave_steps = 1000\nnum_train_epochs = 5\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:26.705929Z","iopub.execute_input":"2025-08-04T14:22:26.706242Z","iopub.status.idle":"2025-08-04T14:22:26.713336Z","shell.execute_reply.started":"2025-08-04T14:22:26.706217Z","shell.execute_reply":"2025-08-04T14:22:26.712412Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/training-data/train_policy.parquet\")\ndf.iloc[10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:28.046663Z","iopub.execute_input":"2025-08-04T14:22:28.047259Z","iopub.status.idle":"2025-08-04T14:22:28.137747Z","shell.execute_reply.started":"2025-08-04T14:22:28.047232Z","shell.execute_reply":"2025-08-04T14:22:28.137055Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"prompt    SUBREDDIT: r/AskReddit\\nTITLE: Cock blocked by...\nlabel     Redhead in one of my classes was driven away b...\nName: 10, dtype: object"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"#### TLDRDataset Class – Custom Dataset for Training Policy model for human evalation\n\nThe `TLDRDataset` class is a custom PyTorch dataset designed for policy model. \nIt loads a `.parquet` file containing prompt/label pairs and tokenizes both the inputs and labels for training.","metadata":{}},{"cell_type":"code","source":"class TLDRDataset(Dataset):\n    def __init__(self, train_path, tokenizer, split, max_length):\n        dataset = pd.read_parquet(train_path)\n        self.post_list = []\n        self.labels = []\n        for sample in dataset.iterrows():\n            self.post_list.append(sample[1][\"prompt\"])\n            self.labels.append(sample[1][\"label\"])\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.input_ids = []\n        self.attn_masks = []\n        \n    def __len__(self):\n        return len(self.post_list)\n    \n    def __getitem__(self, idx):\n        txt = self.post_list[idx]\n        label = self.labels[idx]\n        \n        encodings_dict = self.tokenizer(\n            txt,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\"\n        )\n        encodings_dict_label = self.tokenizer(\n            label,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\"\n        )\n        \n        input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n        attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n        label_ids = torch.tensor(encodings_dict_label[\"input_ids\"])\n        \n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attn_masks,\n            \"labels\": label_ids\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:30.525923Z","iopub.execute_input":"2025-08-04T14:22:30.526227Z","iopub.status.idle":"2025-08-04T14:22:30.533054Z","shell.execute_reply.started":"2025-08-04T14:22:30.526203Z","shell.execute_reply":"2025-08-04T14:22:30.532396Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Tokenizer & Model Setup","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:33.004742Z","iopub.execute_input":"2025-08-04T14:22:33.005035Z","iopub.status.idle":"2025-08-04T14:22:34.224573Z","shell.execute_reply.started":"2025-08-04T14:22:33.005013Z","shell.execute_reply":"2025-08-04T14:22:34.223832Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#### Load Dataset for Training","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/training-data/train_policy.parquet\"\ntrain_dataset = TLDRDataset(\n    train_path=data_path,\n    tokenizer=tokenizer,\n    split=\"train\",\n    max_length=256\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:35.865937Z","iopub.execute_input":"2025-08-04T14:22:35.866248Z","iopub.status.idle":"2025-08-04T14:22:36.169267Z","shell.execute_reply.started":"2025-08-04T14:22:35.866225Z","shell.execute_reply":"2025-08-04T14:22:36.168662Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#### Start Training","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=learning_rate,\n    per_device_train_batch_size=train_batch_size,\n    fp16=False,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    num_train_epochs=2,\n    warmup_steps=50,\n    logging_steps=20,\n    max_steps=2,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:42.836337Z","iopub.execute_input":"2025-08-04T14:22:42.837024Z","iopub.status.idle":"2025-08-04T14:22:53.009254Z","shell.execute_reply.started":"2025-08-04T14:22:42.836995Z","shell.execute_reply":"2025-08-04T14:22:53.008313Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:06, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2, training_loss=5.040317058563232, metrics={'train_runtime': 9.3143, 'train_samples_per_second': 6.871, 'train_steps_per_second': 0.215, 'total_flos': 11806697324544.0, 'train_loss': 5.040317058563232, 'epoch': 0.00975609756097561})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Train reward model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/summarization_policy\")\ntokenizer.save_pretrained(\"/kaggle/working/summarization_policy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:53.011036Z","iopub.execute_input":"2025-08-04T14:22:53.011393Z","iopub.status.idle":"2025-08-04T14:22:55.261506Z","shell.execute_reply.started":"2025-08-04T14:22:53.011364Z","shell.execute_reply":"2025-08-04T14:22:55.260682Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/summarization_policy/tokenizer_config.json',\n '/kaggle/working/summarization_policy/special_tokens_map.json',\n '/kaggle/working/summarization_policy/vocab.json',\n '/kaggle/working/summarization_policy/merges.txt',\n '/kaggle/working/summarization_policy/added_tokens.json',\n '/kaggle/working/summarization_policy/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"MODEL_PATH = \"./summarization_policy\"\nDATA_PATH = \"/kaggle/input/training-data/train.parquet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:55.262438Z","iopub.execute_input":"2025-08-04T14:22:55.262734Z","iopub.status.idle":"2025-08-04T14:22:59.882278Z","shell.execute_reply.started":"2025-08-04T14:22:55.262708Z","shell.execute_reply":"2025-08-04T14:22:59.881548Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = pd.read_parquet(DATA_PATH)\ndf = df[:1000]\nraw_dataset = datasets.Dataset.from_pandas(df)\nraw_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:23:01.574507Z","iopub.execute_input":"2025-08-04T14:23:01.574787Z","iopub.status.idle":"2025-08-04T14:23:03.064076Z","shell.execute_reply.started":"2025-08-04T14:23:01.574769Z","shell.execute_reply":"2025-08-04T14:23:03.063401Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\ntokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:22:59.883865Z","iopub.execute_input":"2025-08-04T14:22:59.884155Z","iopub.status.idle":"2025-08-04T14:23:01.573809Z","shell.execute_reply.started":"2025-08-04T14:22:59.884127Z","shell.execute_reply":"2025-08-04T14:23:01.573074Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"#### Formatting function for reward model","metadata":{}},{"cell_type":"code","source":"def formatting_func(examples):\n    kwargs = {\n        \"padding\": \"max_length\",\n        \"truncation\": True,\n        \"max_length\": 256,\n        \"return_tensors\": \"pt\"\n    }\n    \n    prompt_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n    prompt_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n    \n    tokens_chosen = tokenizer.encode_plus(prompt_chosen_response, **kwargs)\n    tokens_rejected = tokenizer.encode_plus(prompt_rejected_response, **kwargs)\n    \n    return {\n        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0],\n        \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0],\n        \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:23:03.064785Z","iopub.execute_input":"2025-08-04T14:23:03.064972Z","iopub.status.idle":"2025-08-04T14:23:03.070259Z","shell.execute_reply.started":"2025-08-04T14:23:03.064957Z","shell.execute_reply":"2025-08-04T14:23:03.069478Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"formatted_dataset = raw_dataset.map(formatting_func)\nformatted_dataset = formatted_dataset.train_test_split()\nformatted_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:23:03.071144Z","iopub.execute_input":"2025-08-04T14:23:03.071399Z","iopub.status.idle":"2025-08-04T14:23:06.168009Z","shell.execute_reply.started":"2025-08-04T14:23:03.071371Z","shell.execute_reply":"2025-08-04T14:23:06.167260Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071c05c258e349bb94439e5042d1b372"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n        num_rows: 750\n    })\n    test: Dataset({\n        features: ['prompt', 'chosen', 'rejected', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n        num_rows: 250\n    })\n})"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"#### training reward model","metadata":{}},{"cell_type":"code","source":"# Use RewardConfig instead of TrainingArguments\ntraining_args = RewardConfig(\n    output_dir=\"./reward-model-checkpoint\",\n    num_train_epochs=2,\n    gradient_accumulation_steps=1,\n    save_strategy=\"steps\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=1,\n    eval_accumulation_steps=1,\n    logging_steps=10,\n    eval_steps=500,\n    save_steps=500,\n    warmup_steps=50,\n    learning_rate=1e-5,\n    save_total_limit=1,\n    use_cpu=True,\n    max_steps=2,\n    report_to=\"none\",\n    disable_dropout=True,  \n    max_length=1024,      \n)\n\ntrainer = RewardTrainer(\n    model=model,\n    processing_class=tokenizer,  \n    args=training_args,\n    train_dataset=formatted_dataset[\"train\"],\n    eval_dataset=formatted_dataset[\"test\"],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:23:06.169865Z","iopub.execute_input":"2025-08-04T14:23:06.170122Z","iopub.status.idle":"2025-08-04T14:24:07.350110Z","shell.execute_reply.started":"2025-08-04T14:23:06.170103Z","shell.execute_reply":"2025-08-04T14:24:07.349238Z"}},"outputs":[{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3753: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2, training_loss=1.1997116804122925, metrics={'train_runtime': 60.4993, 'train_samples_per_second': 0.066, 'train_steps_per_second': 0.033, 'total_flos': 0.0, 'train_loss': 1.1997116804122925, 'epoch': 0.005333333333333333})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/reward-model\")\ntokenizer.save_pretrained(\"/kaggle/working/reward-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:24:07.351002Z","iopub.execute_input":"2025-08-04T14:24:07.351243Z","iopub.status.idle":"2025-08-04T14:24:08.935047Z","shell.execute_reply.started":"2025-08-04T14:24:07.351223Z","shell.execute_reply":"2025-08-04T14:24:08.934161Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/reward-model/tokenizer_config.json',\n '/kaggle/working/reward-model/special_tokens_map.json',\n '/kaggle/working/reward-model/vocab.json',\n '/kaggle/working/reward-model/merges.txt',\n '/kaggle/working/reward-model/added_tokens.json',\n '/kaggle/working/reward-model/tokenizer.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"#### Reward Scoring Function – Comparing Chosen vs Rejected Responses\n\nThis section demonstrates how to **score model responses** after training a reward model. It compares a preferred (chosen) response with a less-preferred (rejected) one, computing their relative preference score using logits.","metadata":{}},{"cell_type":"code","source":"def get_score(model, tokenizer, prompt, response):\n    instructions = tokenizer.encode_plus(\n        prompt,\n        response,\n        padding=\"max_length\",\n        max_length=256,\n        return_tensors=\"pt\",\n        truncation=True\n    )\n    with torch.no_grad():\n        outputs = model(**instructions)\n    logits = outputs[0]\n    \n    return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:24:08.936061Z","iopub.execute_input":"2025-08-04T14:24:08.936383Z","iopub.status.idle":"2025-08-04T14:24:14.366325Z","shell.execute_reply.started":"2025-08-04T14:24:08.936352Z","shell.execute_reply":"2025-08-04T14:24:14.365616Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"prompt = df.iloc[0][\"prompt\"]\nexample_chosen_response = df.iloc[0][\"chosen\"]\nexample_rejected_response = df.iloc[0][\"rejected\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:24:14.368095Z","iopub.execute_input":"2025-08-04T14:24:14.368476Z","iopub.status.idle":"2025-08-04T14:24:14.959502Z","shell.execute_reply.started":"2025-08-04T14:24:14.368448Z","shell.execute_reply":"2025-08-04T14:24:14.958846Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"loss1 = get_score(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    response=example_chosen_response\n)\n\nloss2 = get_score(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=prompt,\n    response=example_rejected_response\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:24:14.960343Z","iopub.execute_input":"2025-08-04T14:24:14.960663Z","iopub.status.idle":"2025-08-04T14:24:19.084368Z","shell.execute_reply.started":"2025-08-04T14:24:14.960639Z","shell.execute_reply":"2025-08-04T14:24:19.083486Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"input_ids = tokenizer.encode_plus(prompt, example_chosen_response, return_tensors=\"pt\", truncation=True)[\"input_ids\"]\ndecoded = tokenizer.decode(input_ids[0], skip_special_tokens=True)\nprint(decoded)\n\nprint(\"Chosen reward:\", loss1.mean().item())\nprint(\"Rejected reward:\", loss2.mean().item())\nprint(\"Reward diff:\", (loss1 - loss2).mean().item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:24:19.085575Z","iopub.execute_input":"2025-08-04T14:24:19.085825Z","iopub.status.idle":"2025-08-04T14:24:19.109773Z","shell.execute_reply.started":"2025-08-04T14:24:19.085806Z","shell.execute_reply":"2025-08-04T14:24:19.108951Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"SUBREDDIT: r/relationships\nTITLE: To admit or not to admit snooping...\nPOST: I [25M] have snooped in the past and copped up to it to my gf [25F] of 6 years.  We talked it through.  It had been a year or two since the last time.  That's an issue I'm working on.\n\nNow she has a new close male work friend.  I won't go into details, but she hides things from me with him and does other things to make me a bit suspicious.  So...I snooped again, and this time, all texts from her new friend have been deleted and I saw a google search for \"how to get over a guy\" near some searches of his name and views of his Facebook profile.\n\nI asked her about this guy, not mentioning the snooping, and she denied any feelings, we talked for a long time about our relationship and she insisted that she only loves me and I mean the world to her, and that she really wants to work towards getting this relationship back out of the rut we've been in (we both work all the time and barely see each other).\n\nI think if I cop to the snooping, we might have a more honest conversation about what's actually going on (if something is) and why she's having these feelings so we can either work through it together (my preference) or move on.  But obviously, it will open the pandora's box of the snooping.\n\nThink it's worth it to admit to the snooping to hopefully get to the bottom of this?TL;DR:  Snooped, found something, should I admit what I found so we can have a more honest conversation about it with less denial on her part?\nChosen reward: -5.3146538734436035\nRejected reward: -5.519906997680664\nReward diff: 0.20525288581848145\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Reward Scoring Example – Interpreting Preference Alignment\n\nIn this example, we applied the reward model to a real-world Reddit post to evaluate two model-generated responses: one preferred (\"chosen\") and one less preferred (\"rejected\").\n\nThe reward model produced the following scores:\n\n- **Chosen reward**: -5.31  \n- **Rejected reward**: -5.52  \n- **Reward difference**: +0.205\n\nEven though both scores are negative (which is common for raw logits-based outputs), the *chosen* response received a higher score, indicating that the reward model correctly identified it as more aligned with human preferences.\n\nThis reward difference will later be used in PPO fine-tuning to guide the policy model toward generating higher-quality, more preferred responses.\n","metadata":{}},{"cell_type":"markdown","source":"# PPO Training","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/working/reward-model\"\nDATA_PATH = \"/kaggle/input/training-data/train.parquet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:28:36.446315Z","iopub.execute_input":"2025-08-04T14:28:36.446924Z","iopub.status.idle":"2025-08-04T14:28:36.450546Z","shell.execute_reply.started":"2025-08-04T14:28:36.446893Z","shell.execute_reply":"2025-08-04T14:28:36.449908Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df = pd.read_parquet(DATA_PATH)\ndataset = datasets.Dataset.from_pandas(df)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:34:50.925696Z","iopub.execute_input":"2025-08-04T14:34:50.926255Z","iopub.status.idle":"2025-08-04T14:34:51.999007Z","shell.execute_reply.started":"2025-08-04T14:34:50.926212Z","shell.execute_reply":"2025-08-04T14:34:51.998339Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 92534\n})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"\n# Set PPO configuration with additional parameters\nconfig = PPOConfig(\n    reward_model_path=MODEL_PATH,\n    learning_rate=1.41e-5,\n    remove_unused_columns=True,\n    # Additional parameters from the list\n    exp_name=\"sentiment_tuning\",  # Custom experiment name\n    model_adapter_name=\"default\",  # Name of train target PEFT adapter\n    ref_adapter_name=\"reference\",  # Name of reference PEFT adapter\n    num_ppo_epochs=1,             # Number of PPO training epochs\n    whiten_rewards=False,           \n    kl_coef=0.01,                  # KL divergence coefficient\n    kl_estimator=\"k1\",             # KL estimator type\n    cliprange=0.3,                 # Policy clip range\n    vf_coef=0.05,                   # Value function coefficient\n    cliprange_value=0.3,           # Value function clip range\n    gamma=0.95,                    # Discount factor\n    lam=0.90,                      # GAE lambda\n    ds3_gather_for_generation=True,  # DeepSpeed ZeRO-3 optimization\n    batch_size=8,                 # Batch size\n    mini_batch_size=2,             # Mini-batch size\n)\n\n# Process dataset\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side=\"left\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    return tokenizer(\n        [\" \" + text for text in examples[\"chosen\"]],\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"max_length\",\n        max_length=32\n    )\n\ndataset = dataset.rename_columns({\"prompt\": \"review\"})\ndataset = dataset.filter(lambda x: len(x[\"review\"]) > 512, batched=False)\ndataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)\ndataset = dataset.map(tokenize_function, batched=True)\ndataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\ndataset = dataset.select(range(100))  # Use first 100 samples\ndataset.set_format(\"pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:34:53.386306Z","iopub.execute_input":"2025-08-04T14:34:53.387088Z","iopub.status.idle":"2025-08-04T14:35:23.007699Z","shell.execute_reply.started":"2025-08-04T14:34:53.387060Z","shell.execute_reply":"2025-08-04T14:35:23.006781Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/92534 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450e4a662c80400bad7cf619420f0386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92486 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c33c9e77904d7caf5fadbd8f01e66d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92486 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e8999668cca4648a52a40f7f20868b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92486 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b9990eec454d0497620de34ed6ac78"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_PATH).to(device)\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_PATH).to(device)\nreward_model_module = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\nvalue_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1).to(device)\n\nmodel.config.return_dict = True\nmodel_ref.config.return_dict = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:08.029650Z","iopub.execute_input":"2025-08-04T14:56:08.030407Z","iopub.status.idle":"2025-08-04T14:56:09.963765Z","shell.execute_reply.started":"2025-08-04T14:56:08.030379Z","shell.execute_reply":"2025-08-04T14:56:09.962756Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPTBigCodeForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/reward-model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of GPTBigCodeForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/reward-model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:12.938937Z","iopub.execute_input":"2025-08-04T14:56:12.939253Z","iopub.status.idle":"2025-08-04T14:56:13.104503Z","shell.execute_reply.started":"2025-08-04T14:56:12.939231Z","shell.execute_reply":"2025-08-04T14:56:13.103653Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nreward_model_module = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\nreward_model_module = reward_model_module.to(device)\nreward_model_module.eval()  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:14.199952Z","iopub.execute_input":"2025-08-04T14:56:14.200582Z","iopub.status.idle":"2025-08-04T14:56:14.678795Z","shell.execute_reply.started":"2025-08-04T14:56:14.200558Z","shell.execute_reply":"2025-08-04T14:56:14.678094Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPTBigCodeForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/reward-model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"GPTBigCodeForSequenceClassification(\n  (transformer): GPTBigCodeModel(\n    (wte): Embedding(49152, 768)\n    (wpe): Embedding(8192, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x GPTBigCodeBlock(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTBigCodeAttention(\n          (c_attn): Linear(in_features=768, out_features=896, bias=True)\n          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPTBigCodeMLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          (act): PytorchGELUTanh()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"import copy\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, load_in_4bit=False).to(device)\n\nmodel.config.return_dict = True\nmodel_ref.config.return_dict = True\n\nmodel_ref = copy.deepcopy(model).eval()\nfor p in model_ref.parameters(): p.requires_grad = False\n\n# sync generation config\nfrom transformers import GenerationConfig\ngen_cfg = GenerationConfig.from_model_config(model.config)\ngen_cfg.eos_token_id = gen_cfg.pad_token_id = tokenizer.eos_token_id\nmodel.generation_config = gen_cfg\nmodel_ref.generation_config = gen_cfg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:17.751958Z","iopub.execute_input":"2025-08-04T14:56:17.752694Z","iopub.status.idle":"2025-08-04T14:56:18.306371Z","shell.execute_reply.started":"2025-08-04T14:56:17.752663Z","shell.execute_reply":"2025-08-04T14:56:18.305751Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"value_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1).to(device).eval()\nprint(value_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:20.109934Z","iopub.execute_input":"2025-08-04T14:56:20.110731Z","iopub.status.idle":"2025-08-04T14:56:20.580454Z","shell.execute_reply.started":"2025-08-04T14:56:20.110706Z","shell.execute_reply":"2025-08-04T14:56:20.579808Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPTBigCodeForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/reward-model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"GPTBigCodeForSequenceClassification(\n  (transformer): GPTBigCodeModel(\n    (wte): Embedding(49152, 768)\n    (wpe): Embedding(8192, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x GPTBigCodeBlock(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTBigCodeAttention(\n          (c_attn): Linear(in_features=768, out_features=896, bias=True)\n          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPTBigCodeMLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          (act): PytorchGELUTanh()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=1, bias=False)\n)\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom trl import PPOTrainer\nfrom torch.optim import AdamW\n\n# — 1. Ensure reward_model is nn.Module —\nreward_model_module = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device).eval()\n\n# — 2. Build optimizers tuple —\noptimizer_policy = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\noptimizers = (optimizer_policy, None)  # or add scheduler\n\n# — 3. Construct the trainer —\nppo_trainer = PPOTrainer(\n    args             = config,                 # PPOConfig instance\n    processing_class = tokenizer,              # not “tokenizer=…”, but “processing_class=…”\n    model            = model,                  # AutoModelWithValueHead\n    ref_model        = model_ref,              # frozen copy\n    reward_model     = reward_model_module,    # must be nn.Module\n    train_dataset    = train_dataset,          # HF Dataset\n    optimizers       = optimizers,\n    value_model = value_model,\n    callbacks        = [],                     # customize if needed\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:22.297267Z","iopub.execute_input":"2025-08-04T14:56:22.298050Z","iopub.status.idle":"2025-08-04T14:56:22.804828Z","shell.execute_reply.started":"2025-08-04T14:56:22.298022Z","shell.execute_reply":"2025-08-04T14:56:22.804169Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPTBigCodeForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/reward-model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"print(\"Policy model device:\", next(model.parameters()).device)\nprint(\"Reward model device:\", next(reward_model_module.parameters()).device)\nprint(\"Value model device:\", next(value_model.parameters()).device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:24.240016Z","iopub.execute_input":"2025-08-04T14:56:24.240868Z","iopub.status.idle":"2025-08-04T14:56:24.245851Z","shell.execute_reply.started":"2025-08-04T14:56:24.240836Z","shell.execute_reply":"2025-08-04T14:56:24.245193Z"}},"outputs":[{"name":"stdout","text":"Policy model device: cuda:0\nReward model device: cuda:0\nValue model device: cuda:0\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"print(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device count:\", torch.cuda.device_count())\nprint(\"Device name:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:25.649844Z","iopub.execute_input":"2025-08-04T14:56:25.650412Z","iopub.status.idle":"2025-08-04T14:56:25.655099Z","shell.execute_reply.started":"2025-08-04T14:56:25.650383Z","shell.execute_reply":"2025-08-04T14:56:25.654357Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nDevice count: 2\nDevice name: Tesla T4\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:27.535783Z","iopub.execute_input":"2025-08-04T14:56:27.536576Z","iopub.status.idle":"2025-08-04T14:56:28.027364Z","shell.execute_reply.started":"2025-08-04T14:56:27.536544Z","shell.execute_reply":"2025-08-04T14:56:28.026337Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Mon Aug  4 14:56:27 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   77C    P0             33W /   70W |   10297MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0             29W /   70W |     103MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"ppo_trainer.train()\nppo_trainer.save_model(\"ppo_out\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:56:29.910164Z","iopub.execute_input":"2025-08-04T14:56:29.910555Z"}},"outputs":[{"name":"stdout","text":"===training policy===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}