{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5230f3",
   "metadata": {},
   "source": [
    "# Look into the Open-Platypus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6740d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fields: ['input', 'output', 'instruction', 'data_source']\n",
      "\n",
      "Sample row:\n",
      "{'input': '', 'output': 'To find the probability of the spinner landing on $C$, I need to subtract the probabilities of the spinner landing on $A$ and $B$ from $1$, since the sum of the probabilities of all possible outcomes is $1$. I can write this as an equation: $P(C) = 1 - P(A) - P(B)$. I know that $P(A) = \\\\frac{1}{3}$ and $P(B) = \\\\frac{5}{12}$, so I can plug those values into the equation and simplify. I get: $P(C) = 1 - \\\\frac{1}{3} - \\\\frac{5}{12} = \\\\frac{12}{12} - \\\\frac{4}{12} - \\\\frac{5}{12} = \\\\frac{3}{12}$. I can reduce this fraction by dividing the numerator and denominator by $3$, and I get: $P(C) = \\\\frac{1}{4}$. ', 'instruction': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.', 'data_source': 'MATH/PRM-800K'}\n",
      "\n",
      "DataFrame shape: (249, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>To find the probability of the spinner landing...</td>\n",
       "      <td>A board game spinner is divided into three par...</td>\n",
       "      <td>MATH/PRM-800K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>I need to choose 6 people out of 14, and the o...</td>\n",
       "      <td>My school's math club has 6 boys and 8 girls. ...</td>\n",
       "      <td>MATH/PRM-800K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>First we count the number of all 4-letter word...</td>\n",
       "      <td>How many 4-letter words with at least one cons...</td>\n",
       "      <td>MATH/PRM-800K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>She can do this if and only if at least one of...</td>\n",
       "      <td>Melinda will roll two standard six-sided dice ...</td>\n",
       "      <td>MATH/PRM-800K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Think of the problem as a sequence of H's and ...</td>\n",
       "      <td>Let $p$ be the probability that, in the proces...</td>\n",
       "      <td>MATH/PRM-800K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input                                             output  \\\n",
       "0        To find the probability of the spinner landing...   \n",
       "1        I need to choose 6 people out of 14, and the o...   \n",
       "2        First we count the number of all 4-letter word...   \n",
       "3        She can do this if and only if at least one of...   \n",
       "4        Think of the problem as a sequence of H's and ...   \n",
       "\n",
       "                                         instruction    data_source  \n",
       "0  A board game spinner is divided into three par...  MATH/PRM-800K  \n",
       "1  My school's math club has 6 boys and 8 girls. ...  MATH/PRM-800K  \n",
       "2  How many 4-letter words with at least one cons...  MATH/PRM-800K  \n",
       "3  Melinda will roll two standard six-sided dice ...  MATH/PRM-800K  \n",
       "4  Let $p$ be the probability that, in the proces...  MATH/PRM-800K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load a small sample\n",
    "dataset = load_dataset(\"garage-bAInd/Open-Platypus\", split=\"train[:1%]\")\n",
    "\n",
    "# View keys and one sample\n",
    "print(\"Available fields:\", dataset.column_names)\n",
    "print(\"\\nSample row:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Convert to DataFrame for easier inspection (optional)\n",
    "df = pd.DataFrame(dataset)\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79957d7",
   "metadata": {},
   "source": [
    "instruction : the actual prompt or question (what the user would say).\n",
    "\n",
    "input : optional extra context or data (used only when needed).\n",
    "\n",
    "output : the modelâ€™s desired response.\n",
    "\n",
    "data_source : where the sample was sourced from (e.g., MATH/PRM-800K)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948c23e",
   "metadata": {},
   "source": [
    "# Test supervised finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5479b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a76bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selu/anaconda3/envs/sf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3070 Laptop GPU. Num GPUs = 1. Max memory: 7.664 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Generated:\n",
      " ### Instruction:\n",
      "Explain reinforcement learning in simple terms.\n",
      "### Response:\n",
      "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. The agent doesn't need to be told what to do; instead, it learns through trial and error, receiving rewards or penalties for its actions. Over time, the agent improves its policies by choosing actions that yield the highest rewards.\n",
      "\n",
      "Key components of RL:\n",
      "1. **Agent**: The decision-maker that interacts with the environment.\n",
      "2. **Environment**: The world where the agent takes actions and receives rewards.\n",
      "3. **State**: A representation of the current situation of the agent and environment.\n",
      "4. **Action**: A possible move or step the agent can take.\n",
      "5. **Reward**: Feedback received after an action is taken.\n",
      "6. **Policy**: A strategy that defines what the agent should do in different states.\n",
      "7. **Value Function**: Estimates the expected cumulative reward from a given state.\n",
      "8. **Model**: A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import BitsAndBytesConfig  # Import BitsAndBytesConfig\n",
    "\n",
    "# Paths\n",
    "base_model    = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "adapter_dir   = \"./platypus_supervised_fine_tuning/sft-lora\"\n",
    "tokenizer_dir = \"./platypus_supervised_fine_tuning/tokenizer\"\n",
    "\n",
    "# Create quantization config with CPU offloading\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # Correct placement\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load model with quantization config\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    base_model,\n",
    "    max_seq_length=1024,\n",
    "    quantization_config=quantization_config,  # Use config here\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter weights\n",
    "model.load_adapter(adapter_dir)\n",
    "\n",
    "# Prompt\n",
    "instruction = \"Explain reinforcement learning in simple terms.\"\n",
    "prompt = f\"### Instruction:\\n{instruction}\\n### Response:\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "print(\"\\nGenerated:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b212515",
   "metadata": {},
   "source": [
    "# Test DPO'S finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579477d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selu/anaconda3/envs/sf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3070 Laptop GPU. Num GPUs = 1. Max memory: 7.664 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selu/anaconda3/envs/sf/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DPO-Tuned Response:\n",
      "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions and learning from the rewards or penalties it receives. The agent's goal is to maximize its cumulative reward over time. The environment provides feedback in the form of rewards (good outcomes) or penalties (bad outcomes), and the agent uses this feedback to adjust its behavior through a process of trial and error.\n",
      "\n",
      "======== SFT Model Response ========\n",
      "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions and learning from the rewards or penalties it receives. The agent's goal is to maximize the cumulative reward over time. It operates in an environment where the agent interacts with the world, and each interaction results in a state and a reward. The agent's actions determine the next state, and the reward tells the agent how good or bad the action was. Over time, the agent learns which actions lead to higher rewards, improving its performance.\n",
      "\n",
      "### Additional Notes:\n",
      "- **Reinforcement learning vs. Supervised learning**: In supervised learning, the model is trained on labeled data. In reinforcement learning, the model learns through trial and error, receiving rewards and penalties as it goes.\n",
      "- **Reinforcement learning vs. Unsupervised learning**: Unsupervised learning finds patterns in unlabeled data. Reinforcement learning focuses on learning through interactions with an environment, aiming to maximize cumulative reward.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Paths\n",
    "base_model    = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "sft_adapter   = \"./platypus_supervised_fine_tuning/sft-lora\"\n",
    "dpo_adapter   = \"./platypus_supervised_fine_tuning/dpo_final\"\n",
    "tokenizer_dir = \"./platypus_supervised_fine_tuning/dpo_final/tokenizer\"\n",
    "\n",
    "# Load base model with CPU offloading\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model,\n",
    "    max_seq_length = 1024,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    "    device_map = \"auto\",\n",
    "    max_memory = {0: \"6GB\", \"cpu\": \"20GB\"}  # Adjusted for your 8GB GPU\n",
    ")\n",
    "\n",
    "# First load SFT adapter with unique name\n",
    "model.load_adapter(sft_adapter, adapter_name=\"sft\")\n",
    "\n",
    "# Then load DPO adapter with unique name\n",
    "model.load_adapter(dpo_adapter, adapter_name=\"dpo\")\n",
    "\n",
    "# Set the DPO adapter as active\n",
    "model.set_adapter(\"dpo\")\n",
    "\n",
    "# Prompt\n",
    "instruction = \"Explain reinforcement learning in simple terms.\"\n",
    "prompt = f\"### Instruction:\\n{instruction}\\n### Response:\\n\"\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "# Print just the response portion\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "try:\n",
    "    response = full_output.split(\"### Response:\")[1].strip()\n",
    "    print(\"\\nDPO-Tuned Response:\\n\" + response)\n",
    "except IndexError:\n",
    "    print(\"\\nFull Output:\\n\" + full_output)\n",
    "\n",
    "# Compare with SFT model only\n",
    "print(\"\\n======== SFT Model Response ========\")\n",
    "model.set_adapter(\"sft\")  # Switch to SFT adapter\n",
    "sft_outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "sft_full = tokenizer.decode(sft_outputs[0], skip_special_tokens=True)\n",
    "try:\n",
    "    sft_response = sft_full.split(\"### Response:\")[1].strip()\n",
    "    print(sft_response)\n",
    "except IndexError:\n",
    "    print(sft_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19884380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d10474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4782671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
